#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import re
import json
import requests
from datetime import datetime
from collections import Counter

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from utils.logger import setup_logger
from config.config import LLM_CONFIG

logger = setup_logger(__name__)


class LogAnalyzer:
    def __init__(self, ai_config=None):
        self.ai_config = ai_config or LLM_CONFIG
        self.default_error_keywords = [
            'error', 'ERROR', 'Error',
            'exception', 'Exception', 'EXCEPTION',
            'fail', 'FAIL', 'Failed', 'FAILED',
            'timeout', 'TIMEOUT', 'Timeout',
            'connection refused', 'connection failed',
            'unable to connect', 'cannot connect',
            'permission denied', 'access denied',
            'no such file', 'file not found',
            'out of memory', 'memory error',
            'disk full', 'no space left',
            'traceback', 'Traceback',
            'warning', 'WARNING', 'Warning',
            'critical', 'CRITICAL', 'Critical',
            'fatal', 'FATAL', 'Fatal'
        ]

    def read_file_content(self, file_path, line_limit=1000, encoding='utf-8'):
        try:
            print(f"    ğŸ“ éªŒè¯æ—¥å¿—æ–‡ä»¶è·¯å¾„: {file_path}")
            
            if not os.path.exists(file_path):
                print(f"    âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                }

            file_size = os.path.getsize(file_path)
            file_stat = os.stat(file_path)
            
            print(f"    ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            print(f"    ğŸ“… ä¿®æ”¹æ—¶é—´: {datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"    ğŸ“– å¼€å§‹è¯»å–æ—¥å¿—å†…å®¹ï¼Œé™åˆ¶è¡Œæ•°: {line_limit}")

            content_lines = []
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                for i, line in enumerate(f):
                    if i >= line_limit:
                        break
                    content_lines.append(line.rstrip('\n\r'))

            print(f"    âœ… æ—¥å¿—æ–‡ä»¶è¯»å–å®Œæˆï¼Œå®é™…è¯»å– {len(content_lines)} è¡Œ")

            return {
                "success": True,
                "file_path": file_path,
                "file_size": file_size,
                "file_modified": datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                "lines_read": len(content_lines),
                "lines_limited": len(content_lines) >= line_limit,
                "content": content_lines
            }

        except UnicodeDecodeError:
            print(f"    ğŸ”„ UTF-8ç¼–ç å¤±è´¥ï¼Œå°è¯•GBKç¼–ç ...")
            try:
                with open(file_path, 'r', encoding='gbk', errors='ignore') as f:
                    content_lines = []
                    for i, line in enumerate(f):
                        if i >= line_limit:
                            break
                        content_lines.append(line.rstrip('\n\r'))

                print(f"    âœ… GBKç¼–ç è¯»å–æˆåŠŸï¼Œè·å– {len(content_lines)} è¡Œ")

                return {
                    "success": True,
                    "file_path": file_path,
                    "encoding_used": "gbk",
                    "lines_read": len(content_lines),
                    "content": content_lines
                }
            except Exception as e:
                print(f"    âŒ å¤šç§ç¼–ç å°è¯•å¤±è´¥: {e}")
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶ç¼–ç è¯»å–å¤±è´¥: {str(e)}"
                }
        except Exception as e:
            print(f"    âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"
            }

    def find_error_patterns(self, content_lines, error_keywords=None, context_lines=3):
        print(f"    ğŸ” å¼€å§‹é”™è¯¯æ¨¡å¼è¯†åˆ«å’Œå…³é”®è¯åŒ¹é…...")
        
        if error_keywords is None:
            error_keywords = self.default_error_keywords

        error_matches = []
        keyword_counts = Counter()

        for i, line in enumerate(content_lines):
            line_lower = line.lower()

            found_keywords = []
            for keyword in error_keywords:
                if keyword.lower() in line_lower:
                    found_keywords.append(keyword)
                    keyword_counts[keyword] += 1

            if found_keywords:
                start_idx = max(0, i - context_lines)
                end_idx = min(len(content_lines), i + context_lines + 1)
                context = content_lines[start_idx:end_idx]

                error_matches.append({
                    'line_number': i + 1,
                    'line_content': line,
                    'found_keywords': found_keywords,
                    'context': context,
                    'context_start_line': start_idx + 1
                })

        print(f"    âœ… é”™è¯¯æ¨¡å¼è¯†åˆ«å®Œæˆï¼Œå‘ç° {len(error_matches)} ä¸ªé”™è¯¯åŒ¹é…é¡¹")

        return {
            'error_matches': error_matches,
            'keyword_counts': dict(keyword_counts),
            'total_errors': len(error_matches)
        }

    def analyze_log_patterns(self, content_lines):
        print(f"    ğŸ“ˆ å¼€å§‹æ—¥å¿—æ¨¡å¼åˆ†æå’Œç‰¹å¾æå–...")
        
        patterns = {
            'timestamp_formats': [],
            'log_levels': Counter(),
            'ip_addresses': [],
            'common_services': Counter(),
            'file_extensions': Counter()
        }

        timestamp_patterns = [
            r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}',
            r'\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2}',
            r'\w{3} \d{2} \d{2}:\d{2}:\d{2}',
        ]

        ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
        log_level_pattern = r'\b(DEBUG|INFO|WARN|WARNING|ERROR|FATAL|CRITICAL)\b'

        sample_size = min(100, len(content_lines))
        print(f"    ğŸ”¬ åˆ†ææ ·æœ¬å¤§å°: {sample_size} è¡Œ")
        
        for line in content_lines[:sample_size]:
            for pattern in timestamp_patterns:
                if re.search(pattern, line):
                    patterns['timestamp_formats'].append(pattern)
                    break

            ips = re.findall(ip_pattern, line)
            patterns['ip_addresses'].extend(ips)

            levels = re.findall(log_level_pattern, line, re.IGNORECASE)
            for level in levels:
                patterns['log_levels'][level.upper()] += 1

            services = ['mysql', 'nginx', 'apache', 'ssh', 'docker', 'systemd', 'kernel']
            for service in services:
                if service in line.lower():
                    patterns['common_services'][service] += 1

        patterns['timestamp_formats'] = list(set(patterns['timestamp_formats']))
        patterns['ip_addresses'] = list(set(patterns['ip_addresses']))
        patterns['log_levels'] = dict(patterns['log_levels'])
        patterns['common_services'] = dict(patterns['common_services'])

        print(f"    âœ… æ—¥å¿—æ¨¡å¼åˆ†æå®Œæˆ")
        print(f"        ğŸ• æ—¶é—´æˆ³æ ¼å¼: {len(patterns['timestamp_formats'])} ç§")
        print(f"        ğŸ“Š æ—¥å¿—çº§åˆ«: {len(patterns['log_levels'])} ç§")
        print(f"        ğŸŒ IPåœ°å€: {len(patterns['ip_addresses'])} ä¸ª")
        print(f"        ğŸ”§ æ¶‰åŠæœåŠ¡: {len(patterns['common_services'])} ä¸ª")

        return patterns

    def call_ai_analysis(self, prompt, temperature=0.7, max_tokens=1500, timeout=60):
        try:
            print(f"    ğŸ§  å¯åŠ¨AIæ™ºèƒ½åˆ†æå¼•æ“...")
            
            url = f"{self.ai_config['base_url']}{self.ai_config['chat_endpoint']}"
            payload = {
                "model": self.ai_config['model_name'],
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
                "max_tokens": max_tokens
            }

            headers = {'Content-Type': 'application/json'}
            
            print(f"    ğŸŒ å‘AIè¿ç»´å¤§è„‘å‘é€æ—¥å¿—åˆ†æè¯·æ±‚...")
            response = requests.post(url, json=payload, headers=headers, timeout=timeout)

            if response.status_code == 200:
                response_data = response.json()
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    print(f"    âœ… AIåˆ†æå®Œæˆï¼Œç”Ÿæˆä¸“ä¸šè¯Šæ–­æŠ¥å‘Š")
                    return response_data['choices'][0]['message']['content']
                else:
                    print(f"    âŒ AIå“åº”æ ¼å¼å¼‚å¸¸: {response_data}")
                    return f"AIåˆ†æå“åº”æ ¼å¼é”™è¯¯: {response_data}"
            else:
                print(f"    âŒ AIåˆ†æè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return f"AIåˆ†æè¯·æ±‚å¤±è´¥: HTTP {response.status_code}, {response.text}"
        except requests.exceptions.Timeout:
            print(f"    â° AIåˆ†æè¶…æ—¶")
            return "AIåˆ†æè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAIæœåŠ¡çŠ¶æ€"
        except requests.exceptions.ConnectionError:
            print(f"    ğŸŒ AIåˆ†æè¿æ¥å¤±è´¥")
            return "AIåˆ†æè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥AIæœåŠ¡åœ°å€å’Œç«¯å£"
        except Exception as e:
            print(f"    ğŸ’¥ AIåˆ†æå¼‚å¸¸: {e}")
            return f"AIåˆ†æå¤±è´¥: {str(e)}"

    def analyze_log(self, file_path, line_limit=1000, error_keywords=None,
                    context_lines=3, ai_analysis=True, ai_temperature=0.7, ai_max_tokens=1500):
        try:
            print(f"    ğŸš€ å¯åŠ¨æ™ºèƒ½æ—¥å¿—åˆ†æç³»ç»Ÿ...")
            
            file_result = self.read_file_content(file_path, line_limit)
            if not file_result['success']:
                return file_result

            content_lines = file_result['content']
            print(f"    ğŸ” å¼€å§‹å¤šç»´åº¦æ—¥å¿—åˆ†æ...")
            
            error_analysis = self.find_error_patterns(content_lines, error_keywords, context_lines)
            log_patterns = self.analyze_log_patterns(content_lines)

            ai_result = None
            if ai_analysis:
                print(f"    ğŸ§  å‡†å¤‡AIæ·±åº¦åˆ†ææ•°æ®...")
                
                sample_errors = error_analysis['error_matches'][:10]
                sample_content = content_lines[:50]

                prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ—¥å¿—æ–‡ä»¶å†…å®¹ï¼š

=== æ–‡ä»¶ä¿¡æ¯ ===
æ–‡ä»¶è·¯å¾„: {file_path}
æ–‡ä»¶å¤§å°: {file_result.get('file_size', 0)} å­—èŠ‚
ä¿®æ”¹æ—¶é—´: {file_result.get('file_modified', 'Unknown')}
è¯»å–è¡Œæ•°: {len(content_lines)} è¡Œ

=== é”™è¯¯ç»Ÿè®¡ ===
æ€»é”™è¯¯æ•°: {error_analysis['total_errors']}
é”™è¯¯å…³é”®è¯ç»Ÿè®¡: {error_analysis['keyword_counts']}

=== æ—¥å¿—æ¨¡å¼ ===
æ—¶é—´æˆ³æ ¼å¼: {log_patterns['timestamp_formats']}
æ—¥å¿—çº§åˆ«: {log_patterns['log_levels']}
æ¶‰åŠæœåŠ¡: {log_patterns['common_services']}
IPåœ°å€: {log_patterns['ip_addresses'][:10]}

=== é”™è¯¯æ ·ä¾‹ ===
{json.dumps(sample_errors, ensure_ascii=False, indent=2)}

=== æ—¥å¿—å†…å®¹æ ·ä¾‹ ===
{chr(10).join(sample_content)}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼åˆ†æï¼š

1. ã€æ—¥å¿—ç±»å‹åˆ¤æ–­ã€‘
   - ç³»ç»Ÿæ—¥å¿—/åº”ç”¨æ—¥å¿—/é”™è¯¯æ—¥å¿—ç­‰
   - ä¸»è¦è®°å½•çš„æœåŠ¡æˆ–ç»„ä»¶

2. ã€é”™è¯¯åˆ†æã€‘
   - ä¸»è¦é”™è¯¯ç±»å‹å’ŒåŸå› 
   - é”™è¯¯çš„ä¸¥é‡ç¨‹åº¦
   - æ˜¯å¦å­˜åœ¨é‡å¤é”™è¯¯

3. ã€é—®é¢˜è¯Šæ–­ã€‘
   - å¯èƒ½çš„æ ¹æœ¬åŸå› 
   - éœ€è¦å…³æ³¨çš„å¼‚å¸¸æ¨¡å¼
   - ç³»ç»Ÿå¥åº·çŠ¶å†µè¯„ä¼°

4. ã€è§£å†³å»ºè®®ã€‘
   - ç«‹å³éœ€è¦å¤„ç†çš„é—®é¢˜
   - é¢„é˜²æ€§æªæ–½
   - ç›‘æ§æ”¹è¿›å»ºè®®

è¯·ä¿æŒä¸“ä¸šæ€§ï¼Œå»ºè®®å…·ä½“å¯æ“ä½œã€‚
"""
                ai_result = self.call_ai_analysis(prompt, ai_temperature, ai_max_tokens)

            print(f"    âœ… æ™ºèƒ½æ—¥å¿—åˆ†æå®Œæˆ")

            return {
                "success": True,
                "file_info": file_result,
                "error_analysis": error_analysis,
                "log_patterns": log_patterns,
                "ai_analysis": ai_result,
                "analysis_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "summary": {
                    "file_path": file_path,
                    "lines_analyzed": len(content_lines),
                    "errors_found": error_analysis['total_errors'],
                    "main_error_types": list(error_analysis['keyword_counts'].keys()),
                    "log_levels_found": list(log_patterns['log_levels'].keys()),
                    "services_mentioned": list(log_patterns['common_services'].keys())
                }
            }

        except Exception as e:
            print(f"    âŒ æ™ºèƒ½æ—¥å¿—åˆ†æç³»ç»Ÿå¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": f"æ—¥å¿—åˆ†æå¼‚å¸¸: {str(e)}",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }


def log_file_analysis(params=None):
    try:
        print("ğŸš€ å¯åŠ¨AIæ™ºèƒ½æ—¥å¿—åˆ†ææœåŠ¡...")
        logger.info("ğŸ“Š å¼€å§‹æ‰§è¡Œæ—¥å¿—æ–‡ä»¶åˆ†æ")

        if not params or 'file_path' not in params:
            print("âŒ ç¼ºå°‘å¿…éœ€å‚æ•°: file_path")
            return {
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å‚æ•°: file_path",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

        file_path = params['file_path']
        line_limit = params.get('line_limit', 1000)
        error_keywords = params.get('error_keywords', [])
        context_lines = params.get('context_lines', 3)
        ai_analysis = params.get('ai_analysis', True)
        ai_temperature = params.get('ai_temperature', 0.7)
        ai_max_tokens = params.get('ai_max_tokens', 1500)

        print(f"ğŸ“ ç›®æ ‡æ—¥å¿—æ–‡ä»¶: {file_path}")
        print(f"ğŸ“Š åˆ†æå‚æ•°é…ç½®:")
        print(f"    ğŸ“– è¯»å–è¡Œæ•°é™åˆ¶: {line_limit}")
        print(f"    ğŸ” ä¸Šä¸‹æ–‡è¡Œæ•°: {context_lines}")
        print(f"    ğŸ§  AIåˆ†æ: {'å¯ç”¨' if ai_analysis else 'ç¦ç”¨'}")
        
        if error_keywords:
            print(f"    ğŸ¯ è‡ªå®šä¹‰å…³é”®è¯: {len(error_keywords)} ä¸ª")

        analyzer = LogAnalyzer()
        result = analyzer.analyze_log(
            file_path=file_path,
            line_limit=line_limit,
            error_keywords=error_keywords,
            context_lines=context_lines,
            ai_analysis=ai_analysis,
            ai_temperature=ai_temperature,
            ai_max_tokens=ai_max_tokens
        )

        if result and result.get('success'):
            print(f"âœ… æ™ºèƒ½æ—¥å¿—åˆ†ææœåŠ¡å®Œæˆ: {file_path}")
            print(f"ğŸ“Š åˆ†æç»“æœ:")
            print(f"    ğŸ“ è¯»å–è¡Œæ•°: {result['analysis_result']['summary']['lines_analyzed']}")
            print(f"    ğŸš¨ å‘ç°é”™è¯¯: {result['analysis_result']['summary']['errors_found']} ä¸ª")
            print(f"    ğŸ”§ æ¶‰åŠæœåŠ¡: {len(result['analysis_result']['summary']['services_mentioned'])} ä¸ª")
            
            logger.info(f"ğŸ“Š æ—¥å¿—æ–‡ä»¶åˆ†æå®Œæˆ: {file_path}")
            return {
                "success": True,
                "message": f"æ—¥å¿—æ–‡ä»¶åˆ†æå®Œæˆ: {file_path}",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "file_path": file_path,
                "analysis_result": result
            }
        else:
            error_msg = result.get('error', 'åˆ†æå¤±è´¥') if result else 'åˆ†æå¤±è´¥'
            print(f"âŒ æ™ºèƒ½æ—¥å¿—åˆ†æå¤±è´¥: {error_msg}")
            logger.error(f"ğŸš¨ æ—¥å¿—æ–‡ä»¶åˆ†æå¤±è´¥: {error_msg}")
            return {
                "success": False,
                "error": f"æ—¥å¿—æ–‡ä»¶åˆ†æå¤±è´¥: {error_msg}",
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

    except Exception as e:
        print(f"âŒ AIæ™ºèƒ½æ—¥å¿—åˆ†ææœåŠ¡å¼‚å¸¸: {e}")
        logger.error(f"ğŸš¨ æ—¥å¿—æ–‡ä»¶åˆ†ææœåŠ¡å¼‚å¸¸: {e}")
        return {
            "success": False,
            "error": f"æ—¥å¿—æ–‡ä»¶åˆ†ææœåŠ¡å¼‚å¸¸: {str(e)}",
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }


if __name__ == "__main__":
    test_params = {
        'file_path': '/var/log/syslog',
        'line_limit': 500,
        'ai_analysis': True
    }
    result = log_file_analysis(test_params)
    print(f"æµ‹è¯•ç»“æœ: {result}")
