#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
from datetime import datetime, timedelta, time as dt_time
import requests
from decimal import Decimal

import sys
import os

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from utils.logger import setup_logger
from utils.database import get_connection
from config.config import LLM_CONFIG

logger = setup_logger(__name__)


def convert_decimal(obj):
    if isinstance(obj, Decimal):
        return float(obj)
    elif isinstance(obj, dict):
        return {k: convert_decimal(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_decimal(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_decimal(item) for item in obj)
    return obj


def safe_float(value, default=0.0):
    try:
        if value is None:
            return default
        return float(value)
    except (ValueError, TypeError):
        return default


def safe_int(value, default=0):
    try:
        if value is None:
            return default
        return int(value)
    except (ValueError, TypeError):
        return default


def get_weekly_date_range():
    today = datetime.now().date()
    end_date = today - timedelta(days=1)
    start_date = end_date - timedelta(days=6)
    start_datetime = datetime.combine(start_date, dt_time.min)
    end_datetime = datetime.combine(end_date, dt_time.max)
    return start_datetime, end_datetime, start_date, end_date


def generate_unique_id(prefix='oas'):
    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
    return f"{prefix}{timestamp}"


def fetch_weekly_server_metrics(conn, start_time, end_time):
    print(f"    ğŸ“Š å¯åŠ¨æ•°æ®åº“è¿æ¥ï¼ŒæŸ¥è¯¢æœåŠ¡å™¨æ€§èƒ½æŒ‡æ ‡...")
    with conn.cursor() as cursor:
        query = """
                SELECT ip, \
                       AVG(CAST(cpu_usage AS DECIMAL(10, 2)))                  as avg_cpu, \
                       AVG(CAST(memory_usage AS DECIMAL(10, 2)))               as avg_memory, \
                       AVG(CAST(disk_usage AS DECIMAL(10, 2)))                 as avg_disk, \
                       COUNT(*)                                                as record_count,
                       SUM(CASE WHEN cpu_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)    as cpu_anomalies,
                       SUM(CASE WHEN memory_status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as memory_anomalies,
                       SUM(CASE WHEN disk_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)   as disk_anomalies,
                       SUM(CASE WHEN network_status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as network_anomalies,
                       SUM(CASE WHEN packet_loss_status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as packet_loss_anomalies,
                       SUM(CASE WHEN user_load_status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as user_load_anomalies
                FROM howso_server_performance_metrics
                WHERE collect_time BETWEEN %s AND %s
                GROUP BY ip
                """
        cursor.execute(query, (start_time, end_time))
        results = cursor.fetchall()
        print(f"    âœ… æœåŠ¡å™¨æ€§èƒ½æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(results)} å°æœåŠ¡å™¨çš„å‘¨ç»Ÿè®¡æ•°æ®")
        return [dict(zip([desc[0] for desc in cursor.description], convert_decimal(row))) for row in results]


def fetch_weekly_service_status(conn, start_time, end_time):
    print(f"    ğŸ”§ æŸ¥è¯¢æœåŠ¡çŠ¶æ€ç›‘æ§æ•°æ®...")
    with conn.cursor() as cursor:
        query = """
                SELECT platform, \
                       server_ip, \
                       service_name,
                       COUNT(*)                                                   as total_checks,
                       SUM(CASE WHEN status != 'æ­£å¸¸' THEN 1 ELSE 0 END)           as anomaly_count,
                       SUM(CASE WHEN process_status = 'æœªè¿è¡Œ' THEN 1 ELSE 0 END) as stop_count
                FROM plat_service_monitoring
                WHERE insert_time BETWEEN %s AND %s
                GROUP BY platform, server_ip, service_name
                """
        cursor.execute(query, (start_time, end_time))
        results = cursor.fetchall()
        print(f"    âœ… æœåŠ¡çŠ¶æ€æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(results)} ä¸ªæœåŠ¡çš„å‘¨ç»Ÿè®¡æ•°æ®")
        return [dict(zip([desc[0] for desc in cursor.description], convert_decimal(row))) for row in results]


def fetch_weekly_nas_pools(conn, start_date, end_date):
    print(f"    ğŸ’¾ æŸ¥è¯¢å­˜å‚¨æ± ç›‘æ§æ•°æ®...")
    with conn.cursor() as cursor:
        query = """
                SELECT server_name, \
                       pool_name, \
                       COUNT(*)                                         as check_count,
                       AVG(CAST(usage_percentage AS DECIMAL(10, 2)))    as avg_usage,
                       SUM(CASE WHEN status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as anomaly_count
                FROM nas_pools_detail
                WHERE inspection_date BETWEEN %s AND %s
                GROUP BY server_name, pool_name
                """
        cursor.execute(query, (start_date, end_date))
        results = cursor.fetchall()
        print(f"    âœ… å­˜å‚¨æ± æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(results)} ä¸ªå­˜å‚¨æ± çš„å‘¨ç»Ÿè®¡æ•°æ®")
        return [dict(zip([desc[0] for desc in cursor.description], convert_decimal(row))) for row in results]


def fetch_weekly_power_monitoring(conn, start_time, end_time):
    try:
        print(f"    âš¡ æŸ¥è¯¢ç”µåŠ›ç›‘æ§æ•°æ®...")
        with conn.cursor() as cursor:
            query = """
                    SELECT COUNT(*)                                                            as record_count,
                           SUM(CASE WHEN battery_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)            as battery_anomalies,
                           SUM(CASE WHEN avg_input_voltage_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)  as input_voltage_anomalies,
                           SUM(CASE WHEN avg_output_voltage_status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as output_voltage_anomalies,
                           SUM(CASE WHEN avg_input_current_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)  as input_current_anomalies,
                           SUM(CASE WHEN avg_output_current_status != 'æ­£å¸¸' THEN 1 ELSE 0 END) as output_current_anomalies,
                           SUM(CASE WHEN avg_temperature_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)    as temperature_anomalies,
                           SUM(CASE WHEN avg_humidity_status != 'æ­£å¸¸' THEN 1 ELSE 0 END)       as humidity_anomalies
                    FROM power_monitoring_avg_data
                    WHERE inspection_time BETWEEN %s AND %s
                    """
            cursor.execute(query, (start_time, end_time))
            results = cursor.fetchall()
            print(f"    âœ… ç”µåŠ›ç›‘æ§æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(results)} æ¡ç”µåŠ›å‘¨ç»Ÿè®¡æ•°æ®")
            return [dict(zip([desc[0] for desc in cursor.description], convert_decimal(row))) for row in results]
    except Exception as e:
        print(f"    âš ï¸ ç”µåŠ›ç›‘æ§æ•°æ®é‡‡é›†é‡åˆ°é—®é¢˜: {e}")
        logger.error(f"ğŸš¨ Error fetching weekly power monitoring data: {e}")
        return []


def prepare_weekly_data_summary(server_metrics, service_status, nas_pools, power_monitoring):
    print(f"    ğŸ“ˆ å¼€å§‹èšåˆå’Œåˆ†æå‘¨åº¦ç›‘æ§æ•°æ®...")
    
    total_servers = len(server_metrics)
    total_cpu_anomalies = sum(safe_int(m.get('cpu_anomalies', 0)) for m in server_metrics)
    total_memory_anomalies = sum(safe_int(m.get('memory_anomalies', 0)) for m in server_metrics)
    total_disk_anomalies = sum(safe_int(m.get('disk_anomalies', 0)) for m in server_metrics)
    total_network_anomalies = sum(safe_int(m.get('network_anomalies', 0)) for m in server_metrics)
    total_packet_loss_anomalies = sum(safe_int(m.get('packet_loss_anomalies', 0)) for m in server_metrics)
    total_user_load_anomalies = sum(safe_int(m.get('user_load_anomalies', 0)) for m in server_metrics)
    avg_cpu = sum(safe_float(m.get('avg_cpu', 0)) for m in server_metrics) / total_servers if total_servers else 0
    avg_memory = sum(safe_float(m.get('avg_memory', 0)) for m in server_metrics) / total_servers if total_servers else 0
    avg_disk = sum(safe_float(m.get('avg_disk', 0)) for m in server_metrics) / total_servers if total_servers else 0

    total_services = len(service_status)
    total_service_anomalies = sum(safe_int(s.get('anomaly_count', 0)) for s in service_status)
    total_service_stops = sum(safe_int(s.get('stop_count', 0)) for s in service_status)
    total_checks = sum(safe_int(s.get('total_checks', 0)) for s in service_status)

    total_nas_pools = len(nas_pools)
    total_nas_anomalies = sum(safe_int(p.get('anomaly_count', 0)) for p in nas_pools)
    avg_usage = sum(safe_float(p.get('avg_usage', 0)) for p in nas_pools) / total_nas_pools if total_nas_pools else 0

    power_data = power_monitoring[0] if power_monitoring else {}
    total_power_records = safe_int(power_data.get('record_count', 0))
    total_battery_anomalies = safe_int(power_data.get('battery_anomalies', 0))
    total_voltage_anomalies = safe_int(power_data.get('input_voltage_anomalies', 0)) + safe_int(
        power_data.get('output_voltage_anomalies', 0))
    total_current_anomalies = safe_int(power_data.get('input_current_anomalies', 0)) + safe_int(
        power_data.get('output_current_anomalies', 0))
    total_env_anomalies = safe_int(power_data.get('temperature_anomalies', 0)) + safe_int(
        power_data.get('humidity_anomalies', 0))

    print(f"    âœ… å‘¨åº¦æ•°æ®èšåˆå®Œæˆï¼Œç”Ÿæˆç»Ÿè®¡åˆ†æç»“æœ")

    return {
        "æœåŠ¡å™¨çŠ¶æ€": {
            "ç›‘æ§æœåŠ¡å™¨æ•°": total_servers,
            "å¹³å‡CPUä½¿ç”¨ç‡": round(avg_cpu, 2),
            "å¹³å‡å†…å­˜ä½¿ç”¨ç‡": round(avg_memory, 2),
            "å¹³å‡ç£ç›˜ä½¿ç”¨ç‡": round(avg_disk, 2),
            "CPUå¼‚å¸¸æ¬¡æ•°": total_cpu_anomalies,
            "å†…å­˜å¼‚å¸¸æ¬¡æ•°": total_memory_anomalies,
            "ç£ç›˜å¼‚å¸¸æ¬¡æ•°": total_disk_anomalies,
            "ç½‘ç»œå¼‚å¸¸æ¬¡æ•°": total_network_anomalies,
            "ä¸¢åŒ…å¼‚å¸¸æ¬¡æ•°": total_packet_loss_anomalies,
            "ç”¨æˆ·è´Ÿè½½å¼‚å¸¸æ¬¡æ•°": total_user_load_anomalies
        },
        "æœåŠ¡çŠ¶æ€": {
            "ç›‘æ§æœåŠ¡æ•°": total_services,
            "æ€»æ£€æŸ¥æ¬¡æ•°": total_checks,
            "æ€»å¼‚å¸¸æ¬¡æ•°": total_service_anomalies,
            "æ€»åœæ­¢æ¬¡æ•°": total_service_stops
        },
        "å­˜å‚¨çŠ¶æ€": {
            "ç›‘æ§å­˜å‚¨æ± æ•°": total_nas_pools,
            "å¹³å‡ä½¿ç”¨ç‡": round(avg_usage, 2),
            "æ€»å¼‚å¸¸æ¬¡æ•°": total_nas_anomalies
        },
        "ç”µåŠ›çŠ¶æ€": {
            "ç›‘æ§è®°å½•æ•°": total_power_records,
            "ç”µæ± å¼‚å¸¸": total_battery_anomalies,
            "ç”µå‹å¼‚å¸¸": total_voltage_anomalies,
            "ç”µæµå¼‚å¸¸": total_current_anomalies,
            "ç¯å¢ƒå¼‚å¸¸": total_env_anomalies
        }
    }


def get_weekly_exceptions(server_metrics, service_status, nas_pools, power_monitoring):
    print(f"    ğŸ” å¼€å§‹è¯†åˆ«å’Œåˆ†æå¼‚å¸¸æ•°æ®...")
    
    exceptions = {
        "é«˜å¼‚å¸¸æœåŠ¡å™¨": [m for m in server_metrics if
                         (safe_int(m.get('cpu_anomalies', 0)) + safe_int(m.get('memory_anomalies', 0)) + safe_int(
                             m.get('disk_anomalies', 0)) + safe_int(m.get('network_anomalies', 0)) + safe_int(
                             m.get('packet_loss_anomalies', 0)) + safe_int(m.get('user_load_anomalies', 0))) > 5],
        "é«˜å¼‚å¸¸æœåŠ¡": [s for s in service_status if safe_int(s.get('anomaly_count', 0)) > 2],
        "é«˜å¼‚å¸¸å­˜å‚¨æ± ": [p for p in nas_pools if safe_int(p.get('anomaly_count', 0)) > 1],
        "ç”µåŠ›ç›‘æ§å¼‚å¸¸": power_monitoring
    }

    filtered_exceptions = {k: v for k, v in exceptions.items() if v}
    print(f"    âœ… å¼‚å¸¸æ•°æ®è¯†åˆ«å®Œæˆï¼Œå‘ç° {len(filtered_exceptions)} ç±»å¼‚å¸¸æƒ…å†µ")
    
    return filtered_exceptions


def parse_ai_response(ai_response):
    print(f"    ğŸ§  å¼€å§‹è§£æAIåˆ†æç»“æœ...")
    
    sections = {
        "è¿ç»´æ—¥æŠ¥": "",
        "å¼‚å¸¸åˆ†æ": "",
        "é£é™©é¢„æµ‹": "",
        "è¿ç»´å»ºè®®": "",
        "é‡ç‚¹å…³æ³¨": "",
        "ä¸­åº¦å…³æ³¨": ""
    }

    pattern = r"#{1,3}\s*(?:\d+\.\s*)?([\u4e00-\u9fa5]+)(?:\s*[ï¼ˆ\(].*[ï¼‰\)])?(?:ï¼ˆ\d+å­—ï¼‰)?\s*([\s\S]*?)(?=#{1,3}\s*(?:\d+\.\s*)?[\u4e00-\u9fa5]+|---+|\n\n\n|$)"
    matches = re.findall(pattern, ai_response)

    for title, content in matches:
        for section in sections.keys():
            if section in title:
                clean_content = content.strip()
                clean_content = re.sub(r'^\s*\d+\.\s*', '', clean_content, flags=re.MULTILINE)
                clean_content = re.sub(r'[\*\-]{1,3}\s+', '', clean_content, flags=re.MULTILINE)
                clean_content = re.sub(r'\n+', ' ', clean_content)
                clean_content = re.sub(r'\s+', ' ', clean_content)
                clean_content = clean_content.replace('---', '')
                sections[section] = clean_content.strip()
                break

    if not any(sections.values()):
        current_section = None
        content_buffer = []
        lines = ai_response.split('\n')
        for line in lines:
            line = line.strip()
            if not line or line.startswith('---'):
                continue
            section_match = False
            for section in sections.keys():
                if section in line and (
                        line.startswith(section) or
                        re.search(r'\d+\.\s+' + section, line) or
                        re.match(r'^#+\s*' + section, line) or
                        re.match(r'^#+\s*\d+\.\s*' + section, line) or
                        re.match(r'^' + section + r'\s*[ï¼ˆ(]', line)
                ):
                    if current_section and content_buffer:
                        sections[current_section] = ' '.join(content_buffer).strip()
                    current_section = section
                    content_buffer = []
                    section_match = True
                    break
            if not section_match and current_section:
                if not line.startswith('#'):
                    content_buffer.append(line)
        if current_section and content_buffer:
            sections[current_section] = ' '.join(content_buffer).strip()

    print(f"    âœ… AIåˆ†æç»“æœè§£æå®Œæˆï¼Œç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š")
    return sections


def get_ai_weekly_analysis(data_summary, exception_data):
    print(f"    ğŸ§  å¯åŠ¨QWEN3-32B AIå¼•æ“è¿›è¡Œå‘¨åº¦æ·±åº¦åˆ†æ...")
    
    prompt = f"""
    è¯·ä½œä¸ºæ•°æ®ä¸­å¿ƒè¿ç»´ä¸“å®¶ï¼Œæ ¹æ®ä»¥ä¸‹è¿‡å»7å¤©çš„æ•°æ®ä¸­å¿ƒç›‘æ§æ•°æ®ï¼Œåˆ†åˆ«æä¾›ä»¥ä¸‹åˆ†æï¼ˆæ³¨æ„å­—æ•°é™åˆ¶ï¼‰ï¼š

    1. è¿ç»´æ—¥æŠ¥(400å­—)ï¼šåŒ…æ‹¬è¿‡å»7å¤©çš„æ•´ä½“è¿è¡ŒçŠ¶å†µæ¦‚è¿°ã€ä¸»è¦æŒ‡æ ‡è¶‹åŠ¿ã€å…³é”®äº‹ä»¶ç­‰
    2. å¼‚å¸¸åˆ†æ(300å­—)ï¼šå¯¹å‘ç°çš„å¼‚å¸¸æƒ…å†µè¿›è¡ŒåŸå› åˆ†æå’Œè¶‹åŠ¿åˆ¤æ–­
    3. é£é™©é¢„æµ‹(300å­—)ï¼šæ ¹æ®ä¸€å‘¨æ•°æ®é¢„æµ‹ä¸‹å‘¨å¯èƒ½å‡ºç°çš„é£é™©
    4. è¿ç»´å»ºè®®(300å­—)ï¼šé’ˆå¯¹å‘ç°çš„é—®é¢˜æå‡ºå…·ä½“å¯è¡Œçš„è¿ç»´å»ºè®®
    5. é‡ç‚¹å…³æ³¨(200å­—)ï¼šä¸‹å‘¨éœ€è¦é‡ç‚¹å…³æ³¨å’Œå¤„ç†çš„é—®é¢˜
    6. ä¸­åº¦å…³æ³¨(200å­—)ï¼šéœ€è¦æŒç»­ç›‘æ§ä½†æš‚ä¸éœ€è¦ç«‹å³å¤„ç†çš„é—®é¢˜

    æ•°æ®æ¦‚å†µï¼š
    {json.dumps(data_summary, ensure_ascii=False, indent=2)}

    å¼‚å¸¸æ•°æ®ï¼š
    {json.dumps(exception_data, ensure_ascii=False, indent=2)}

    è¯·ç¡®ä¿æ¯ä¸ªéƒ¨åˆ†çš„å†…å®¹ä¸“ä¸šã€ç®€æ´ä¸”æœ‰é’ˆå¯¹æ€§ï¼Œä¸è¦è¶…å‡ºå­—æ•°é™åˆ¶ã€‚
    è¯·ç”¨å°æ ‡é¢˜æ ‡ç¤ºæ¯ä¸ªéƒ¨åˆ†ï¼Œç¡®ä¿å¯ä»¥æ¸…æ™°åŒºåˆ†ã€‚
    """

    url = f"{LLM_CONFIG['base_url']}{LLM_CONFIG['chat_endpoint']}"
    payload = {
        "model": LLM_CONFIG['model_name'],
        "messages": [
            {"role": "system",
             "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•°æ®ä¸­å¿ƒè¿ç»´ä¸“å®¶ï¼Œè´Ÿè´£åˆ†æè¿‡å»7å¤©çš„ç›‘æ§æ•°æ®å¹¶æä¾›ä¸“ä¸šçš„è¿ç»´å»ºè®®ã€‚è¯·ç¡®ä¿å›ç­”ä¸­çš„å…­ä¸ªéƒ¨åˆ†ç”¨æ˜ç¡®çš„æ ‡é¢˜éš”å¼€ï¼Œä¾¿äºè§£æã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }

    headers = {"Content-Type": "application/json"}

    try:
        print(f"    ğŸŒ å‘AIè¿ç»´å¤§è„‘å‘é€å‘¨æŠ¥åˆ†æè¯·æ±‚...")
        logger.info("ğŸ”® å‘é€å‘¨æŠ¥AIåˆ†æè¯·æ±‚...")
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()

        result = response.json()
        print(f"    âœ… AIè¿ç»´å¤§è„‘åˆ†æå®Œæˆï¼Œç”Ÿæˆä¸“ä¸šå‘¨æŠ¥")
        logger.info("ğŸ¯ æˆåŠŸæ¥æ”¶å‘¨æŠ¥AIåˆ†æå“åº”")
        ai_response = result["choices"][0]["message"]["content"]

        sections = parse_ai_response(ai_response)
        return sections
    except Exception as e:
        print(f"    âŒ AIè¿ç»´å¤§è„‘è¿æ¥å¼‚å¸¸: {e}")
        logger.error(f"ğŸš¨ å‘¨æŠ¥AIåˆ†æè°ƒç”¨å¤±è´¥: {e}")
        return {
            "è¿ç»´æ—¥æŠ¥": "æ— æ³•è¿æ¥AIåˆ†ææœåŠ¡ï¼Œè¯·æ‰‹åŠ¨åˆ†æç›‘æ§æ•°æ®ã€‚",
            "å¼‚å¸¸åˆ†æ": "",
            "é£é™©é¢„æµ‹": "",
            "è¿ç»´å»ºè®®": "",
            "é‡ç‚¹å…³æ³¨": "",
            "ä¸­åº¦å…³æ³¨": ""
        }


def save_analysis_summary(conn, analysis_data, start_date, exception_data):
    print(f"    ğŸ’¾ å‡†å¤‡å°†å‘¨æŠ¥åˆ†æç»“æœå­˜å‚¨åˆ°æ•°æ®åº“...")
    unique_id = generate_unique_id()

    insert_query = """
                   INSERT INTO operation_analysis_summary
                   (id, report_date, report_type, operation_daily, exception_analysis,
                    exception_data, risk_prediction, operation_suggestion, key_focus,
                    moderate_focus, created_at)
                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \
                   """

    with conn.cursor() as cursor:
        cursor.execute(insert_query, (
            unique_id,
            start_date,
            'weekly',
            analysis_data.get('è¿ç»´æ—¥æŠ¥', ''),
            analysis_data.get('å¼‚å¸¸åˆ†æ', ''),
            json.dumps(exception_data, ensure_ascii=False),
            analysis_data.get('é£é™©é¢„æµ‹', ''),
            analysis_data.get('è¿ç»´å»ºè®®', ''),
            analysis_data.get('é‡ç‚¹å…³æ³¨', ''),
            analysis_data.get('ä¸­åº¦å…³æ³¨', ''),
            datetime.now()
        ))
    conn.commit()
    print(f"    âœ… å‘¨æŠ¥åˆ†æç»“æœå·²æˆåŠŸå­˜å‚¨ï¼Œè®°å½•ID: {unique_id}")
    logger.info(f"ğŸ“Š å‘¨æŠ¥åˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ï¼ŒID: {unique_id}")
    return unique_id


def weekly_monitoring_report(params=None):
    try:
        print("ğŸš€ å¯åŠ¨AIæ™ºèƒ½å‘¨æŠ¥ç”Ÿæˆç³»ç»Ÿ...")
        logger.info("ğŸ“Š å¼€å§‹ç”Ÿæˆå‘¨æŠ¥ç›‘æ§æŠ¥å‘Š...")
        conn = get_connection()

        start_time, end_time, start_date, end_date = get_weekly_date_range()
        print(f"ğŸ“… è®¾å®šå‘¨æŠ¥åˆ†ææ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d')} åˆ° {end_time.strftime('%Y-%m-%d')}")
        logger.info(f"ğŸ“… å‘¨æŠ¥æ—¥æœŸèŒƒå›´: {start_time} åˆ° {end_time}")

        print(f"ğŸ“Š å¼€å§‹é‡‡é›†å¤šç»´åº¦ç›‘æ§æ•°æ®...")
        server_metrics = fetch_weekly_server_metrics(conn, start_time, end_time)
        logger.info(f"ğŸ“ˆ è·å–åˆ° {len(server_metrics)} å°æœåŠ¡å™¨çš„å‘¨æŠ¥æ•°æ®")

        service_status = fetch_weekly_service_status(conn, start_time, end_time)
        logger.info(f"ğŸ”§ è·å–åˆ° {len(service_status)} é¡¹æœåŠ¡çš„å‘¨æŠ¥æ•°æ®")

        nas_pools = fetch_weekly_nas_pools(conn, start_time.date(), end_time.date())
        logger.info(f"ğŸ’¾ è·å–åˆ° {len(nas_pools)} ä¸ªå­˜å‚¨æ± çš„å‘¨æŠ¥æ•°æ®")

        power_monitoring = fetch_weekly_power_monitoring(conn, start_time, end_time)
        logger.info(f"âš¡ è·å–åˆ° {len(power_monitoring)} æ¡ç”µåŠ›ç›‘æ§å‘¨æŠ¥æ•°æ®")

        if not server_metrics and not service_status and not nas_pools and not power_monitoring:
            print("âš ï¸ æœªå‘ç°å¯åˆ†æçš„å‘¨æŠ¥æ•°æ®")
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨äºåˆ†æçš„å‘¨æŠ¥æ•°æ®")
            conn.close()
            return {"success": False, "message": "æ²¡æœ‰å¯ç”¨äºåˆ†æçš„å‘¨æŠ¥æ•°æ®"}

        print(f"ğŸ”§ å¼€å§‹æ•°æ®èšåˆå’Œç»Ÿè®¡åˆ†æ...")
        data_summary = prepare_weekly_data_summary(server_metrics, service_status, nas_pools, power_monitoring)
        exception_data = get_weekly_exceptions(server_metrics, service_status, nas_pools, power_monitoring)

        print(f"ğŸ§  å¯åŠ¨AIæ·±åº¦åˆ†æå¼•æ“...")
        analysis_result = get_ai_weekly_analysis(data_summary, exception_data)
        logger.info("ğŸ¯ å‘¨æŠ¥AIåˆ†æå®Œæˆ")

        print(f"ğŸ’¾ ä¿å­˜åˆ†æç»“æœåˆ°è¿ç»´æ•°æ®åº“...")
        summary_id = save_analysis_summary(conn, analysis_result, start_date, exception_data)

        conn.close()

        result = {
            "success": True,
            "summary_id": summary_id,
            "report_period": f"{start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}",
            "report_type": "weekly",
            "analysis": analysis_result,
            "data_summary": data_summary,
            "exception_count": {
                "high_anomaly_servers": len(exception_data.get("é«˜å¼‚å¸¸æœåŠ¡å™¨", [])),
                "high_anomaly_services": len(exception_data.get("é«˜å¼‚å¸¸æœåŠ¡", [])),
                "high_anomaly_storage": len(exception_data.get("é«˜å¼‚å¸¸å­˜å‚¨æ± ", [])),
                "power_anomalies": len(exception_data.get("ç”µåŠ›ç›‘æ§å¼‚å¸¸", []))
            },
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        print(f"âœ… AIæ™ºèƒ½å‘¨æŠ¥ç”Ÿæˆå®Œæˆ")
        print(f"ğŸ“Š åˆ†æèŒƒå›´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")
        print(f"ğŸ“‹ æŠ¥å‘ŠID: {summary_id}")

        return result
    except Exception as e:
        print(f"âŒ AIæ™ºèƒ½å‘¨æŠ¥ç³»ç»Ÿæ‰§è¡Œå¼‚å¸¸: {e}")
        logger.error(f"ğŸš¨ ç”Ÿæˆå‘¨æŠ¥ç›‘æ§æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {"success": False, "error": str(e)}


if __name__ == "__main__":
    result = weekly_monitoring_report()
    print(f"æµ‹è¯•ç»“æœ: {result}")
